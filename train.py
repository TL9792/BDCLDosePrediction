import shutil
import sys, os
import numpy as np
import time
import torch
import torch.backends.cudnn as cudnn
import torch.nn as nn
import torch.nn.functional as F
from torch import optim
from torch.utils import data
import pandas as pd
import SimpleITK as sitk
from Network.BeamDoseNetwork.ResUnet import RUNet,RUnet_encoder,RUnet_decoder
from provided_code.data_loader import KBPDataset
from Evaluation.DoseScoreDVHScore import EvaluateDose
from provided_code.general_functions import get_paths, make_directory_and_return_path, sparse_vector_function
from MultiBeamVoting import reconstruct
from Loss import loss
# import cv2
import pdb
from torch.utils.tensorboard import SummaryWriter


if __name__ == '__main__':

    train_start_time = time.localtime()
    time_stamp = time.strftime("%Y%m%d%H%M%S", train_start_time)
    
    # Define project directories 
    # TODO: Must define the path of where the data is stored.
    primary_directory = '{}'.format(sys.path[0])
    print(torch.cuda.is_available(), os.environ['CUDA_VISIBLE_DEVICES'])
    
    # Define directory where given data is stored
    training_data_dir = "Data/train-pats/"
    validation_data_dir = "Data/validation-pats/"
    testing_data_dir = "Data/test-pats/"

    # path where any data generated by this code (e.g., predictions, models) are stored
    results_dir = '{}/result'.format(primary_directory)
    os.makedirs(results_dir, exist_ok=True)

    # Name model to train and number of epochs to train it 
    number_of_training_epochs = 1
    epoch_num = 1000   
    batch_size = 2
    lr = 1e-4
    num_workers = 0
    dose_scale_factor = 80.0
    ct_range = [-400,400]
    en = RUnet_encoder(13,9,16)
    de = RUnet_decoder(13,9,16)
    model = RUNet(en,de)
    net = nn.DataParallel(module=model)
    net.cuda()
    model_name = net.module.model_name()
    model_desc = sys.argv[1]

    # Prepare the data directory
    train_data_paths = get_paths(training_data_dir, ext='') 
    validation_data_paths = get_paths(validation_data_dir, ext='') 
    test_data_paths = get_paths(testing_data_dir, ext='') 

    # Make directories for data and models
    model_results_path = '{}/{}_{}'.format(results_dir, model_name, model_desc)
    model_dir = make_directory_and_return_path('{}/models'.format(model_results_path))
    val_prediction_dir = make_directory_and_return_path('{}/validation-predictions'.format(model_results_path))
    loss_file_name = '{}/loss_curve.txt'.format(model_results_path)
    log_file_name = '{}/log.txt'.format(model_results_path)

    # load data
    dataset_train = KBPDataset(train_data_paths, flipped=True, rotate=True, noise=False, deformation=False, ct_range=ct_range)
    data_loader_train = data.DataLoader(
            dataset=dataset_train, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=False, num_workers=num_workers)

    dataset_validation = KBPDataset(validation_data_paths, flipped=False, rotate=False, noise=False, deformation=False, ct_range=ct_range)
    data_loader_validation = data.DataLoader(
            dataset=dataset_validation, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=False, num_workers=num_workers)
    
    dataset_test = KBPDataset(test_data_paths, flipped=False, rotate=False, noise=False, deformation=False, ct_range=ct_range)
    data_loader_test = data.DataLoader(
            dataset=dataset_test, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=False, num_workers=num_workers)

    # initial of optimizer
    optimizer = optim.Adam(
                        net.parameters(),
                        lr=lr,
                        betas=(0.9, 0.999),
                        eps=1e-08,
                        weight_decay=0)


    acc_time = 0
    best_dvh_score = 100
    best_dose_score = 100
    best_model_filename = '{}/epoch_{}.pth.tar'.format(model_dir, 1)

    # print training info
    training_setting_lines = "Training settings:\n\
            Model name: {}\n\
            Model description: {}\n\
            Model parameters number: {}\n\
            Samples (training/validation/testing): {}/{}/{}\n\
            Dose scaling factor: {}\n\
            CT intensity rescaled range: {}\n\
            Epoch num: {}\n\
            Batch size: {}\n\
            Learning rate: {}\n\
            CPU threads: {}\n\
            GPU used: {}\n\
            Start time: {}\n\
            model store directory:{}\n".format(
                    model_name,
                    model_desc, 
                    sum(x.numel() for x in net.parameters()), 
                    len(dataset_train), len(dataset_validation), len(dataset_test), 
                    dose_scale_factor, 
                    ct_range, 
                    epoch_num, batch_size, lr, 
                    num_workers,
                    os.environ['CUDA_VISIBLE_DEVICES'],
                    time.strftime("%Y-%m-%d %H:%M:%S", train_start_time),
                    model_dir)
    print(training_setting_lines)
    log_file = open(log_file_name,'a')
    log_file.write(training_setting_lines)
    log_file.close()

    ## Training stage
    writer = SummaryWriter(model_results_path + '/log')
    net.train()
    for epoch in range(epoch_num):

        t0 = time.perf_counter()

        # training initialization
        print("Training phase ...")
        epoch_loss = 0
        epoch_loss_num = 0
        
        train_evaluator = EvaluateDose(dataset_train)

        # see if eval
        need_eval = ((epoch+1) % 1) == 0

        # train
        for batch_id, batch in enumerate(data_loader_train):

            # fetch data
            img = batch['ct']
            roi = batch['structure_masks']
            ptv = batch['ptv']
            oar = batch['oar']
            direction_mask = batch['direction']
            gt_dose = batch['dose'] / dose_scale_factor
            coarse_dose = batch['coarse_dose'] / dose_scale_factor
            direction_gt = direction_mask * gt_dose
            dose_mask = batch['possible_dose_mask']
            spacing = batch['voxel_dimensions']
            pat_id = batch['patient_id']
            n = len(img)

            # convert to GPU memory
            img = img.cuda()
            roi = roi.cuda()
            ptv = ptv.cuda()
            oar = oar.cuda()
            direction_gt = direction_gt.cuda()
            gt_dose = gt_dose.cuda()
            direction_mask = direction_mask.cuda()
            coarse_dose = coarse_dose.cuda()
            dose_mask = dose_mask.cuda()

            # forward propagation
            pd_dose = net(torch.cat([img, ptv, oar, coarse_dose, direction_mask], dim=1).float())
            pd_dose = pd_dose * direction_mask

            # compute loss
            loss_train, reimg = loss(pd_dose, direction_gt, direction_mask, roi, spacing, coarse_dose, gt_dose, dose_mask)
            reimg = reimg.data.cpu().numpy().copy()
            gt_dose = gt_dose.data.cpu().numpy().copy()

            epoch_loss += n * loss_train.item()

            epoch_loss_num += n
            train_evaluator.append_sample(reimg * dose_scale_factor, batch, gt_dose * dose_scale_factor)

            # backward propagation
            optimizer.zero_grad()
            loss_train.backward()
            optimizer.step()

        # calculate train loss and train metric
        train_loss = epoch_loss / epoch_loss_num

        train_dvh_score, train_dose_score = train_evaluator.make_metrics()
        writer.add_scalar('loss/train', train_loss, epoch)
        writer.add_scalar('dose_score/train_dose_score', train_dose_score, epoch)
        writer.add_scalar('dvh_score/train_dvh_score', train_dvh_score, epoch)

        # print out the training loss and metric
        print('Epoch {0:d}/{1:d} --- Finished.'.format(epoch+1, epoch_num))
        print('Training loss: {:.6f}'.format(train_loss))
        print('For this training epoch:\n'
        '\tthe DVH score is {:.3f}\n '
        '\tthe dose score is {:.3f}'.format(train_dvh_score, train_dose_score))

        ### validation phase
        if need_eval:
            with torch.no_grad():
                print("Validation phase ...")
                net.eval()
                val_epoch_loss_num = 0
                val_epoch_loss = 0
                val_evaluator = EvaluateDose(dataset_validation)

                for batch_id, batch in enumerate(data_loader_validation):

                    # fetch data
                    img = batch['ct']
                    roi = batch['structure_masks']
                    ptv = batch['ptv']
                    oar = batch['oar']
                    direction_mask = batch['direction']
                    gt_dose = batch['dose'] / dose_scale_factor
                    coarse_dose = batch['coarse_dose'] / dose_scale_factor
                    direction_gt = direction_mask * gt_dose
                    dose_mask = batch['possible_dose_mask']
                    spacing = batch['voxel_dimensions']
                    n = len(img)

                    # convert to GPU memory
                    img = img.cuda()
                    roi = roi.cuda()
                    ptv = ptv.cuda()
                    oar = oar.cuda()
                    dose_mask = dose_mask.cuda()
                    direction_gt = direction_gt.cuda()
                    gt_dose = gt_dose.cuda()
                    direction_mask = direction_mask.cuda()
                    coarse_dose = coarse_dose.cuda()

                    # forward propagation
                    oar = oar.squeeze(dim=1)
                    pd_dose = net(torch.cat([img, ptv, oar, coarse_dose, direction_mask], dim=1).float())
                    pd_dose = pd_dose * direction_mask

                    # compute loss
                    loss_val, reimg = loss(pd_dose, direction_gt, direction_mask, roi, spacing, coarse_dose, gt_dose, dose_mask)
                    reimg = reimg.data.cpu().numpy().copy()
                    gt_dose = gt_dose.data.cpu().numpy().copy()

                    val_epoch_loss += n * loss_val.item()
                    val_epoch_loss_num += n
                    val_evaluator.append_sample(reimg * dose_scale_factor, batch, gt_dose * dose_scale_factor)

                # calculate validation metric and loss
                valuation_loss = val_epoch_loss / val_epoch_loss_num

                val_dvh_score, val_dose_score = val_evaluator.make_metrics()
                writer.add_scalar('loss/val', valuation_loss, epoch)
                writer.add_scalar('dose_score/val_dose_score', val_dose_score, epoch)
                writer.add_scalar('dvh_score/val_dvh_score', val_dvh_score, epoch)

                # print out the validation loss and metric
                print('val_loss:',valuation_loss)
                print('For this validation epoch:\n'
                '\tthe DVH score is {:.3f}\n '
                '\tthe dose score is {:.3f}'.format(val_dvh_score, val_dose_score))

                # store model
                if val_dose_score <= best_dose_score:
                    best_dvh_score = val_dvh_score
                    best_dose_score = val_dose_score
                    # remove former best model
                    if os.path.exists(best_model_filename):
                        os.remove(best_model_filename)
                    # save current best model
                    best_model_filename = '{}/epoch_{}.pth.tar'.format(model_dir, epoch + 1)                                
                    torch.save({
                            'epoch':epoch,
                            'acc_time':acc_time,
                            'time_stamp':time_stamp,
                            'best_dvh_score':best_dvh_score,
                            'best_dose_score':best_dose_score,
                            'best_model_filename':best_model_filename,
                            'model_state_dict':net.state_dict(),
                            'optimizer_state_dict':optimizer.state_dict()}, 
                            best_model_filename)
                    print('Best model (epoch = {}) saved.'.format(epoch + 1))


        # calculate epoch time
        t1 = time.perf_counter()
        epoch_t = t1 - t0
        acc_time += epoch_t
        print("Epoch time cost: {h:>02d}:{m:>02d}:{s:>02d}".format(
                h=int(epoch_t) // 3600, m=(int(epoch_t) % 3600) // 60, s=int(epoch_t) % 60))

        # record metric into log file
        if need_eval:
            loss_file_line = '{epoch:>05d}\t{train_loss:>8.6f}\t{dvh_score:>8.6f}\t{dose_score:>8.6f}\t{val_loss:>8.6f}\t{val_dvh_score:>8.6f}\t{val_dose_score:>8.6f}\n'.format(
                        epoch=epoch+1,train_loss=train_loss,dvh_score=train_dvh_score,dose_score=train_dose_score,val_loss=valuation_loss,val_dvh_score=val_dvh_score,val_dose_score=val_dose_score)
        else:
            loss_file_line = '{epoch:>05d}\t{train_loss:>8.6f}\t{dvh_score:>8.6f}\t{dose_score:>8.6f}\n'.format(
                        epoch=epoch+1,train_loss=train_loss,dvh_score=train_dvh_score,dose_score=train_dose_score)
        with open(loss_file_name,'a') as loss_file:
            loss_file.write(loss_file_line)


